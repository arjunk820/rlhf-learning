{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SFT Training for Tweet Generation\n",
        "\n",
        "This notebook implements Supervised Fine-Tuning (SFT) for tweet generation using GPT-2.\n",
        "\n",
        "## Setup and Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q transformers datasets trl wandb accelerate\n",
        "\n",
        "# Import libraries\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import wandb\n",
        "import os\n",
        "import json\n",
        "\n",
        "print(\"‚úÖ Packages installed and imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPU Setup and Device Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"üöÄ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üìä GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    print(f\"üîß CUDA Version: {torch.version.cuda}\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(f\"üçé Using Apple Silicon GPU (MPS)\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(f\"üíª Using CPU (training will be slower)\")\n",
        "\n",
        "print(f\"\\nüéØ Selected device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive (optional - if you want to store data there)\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Check if dataset file exists\n",
        "dataset_path = \"tweet_sft_dataset_10k.jsonl\"\n",
        "\n",
        "if os.path.exists(dataset_path):\n",
        "    print(f\"‚úÖ Found dataset: {dataset_path}\")\n",
        "    data_path = dataset_path\n",
        "else:\n",
        "    print(\"üìù Creating sample dataset for testing...\")\n",
        "    # Create a small sample dataset\n",
        "    sample_data = [\n",
        "        {\"instruction\": \"Write a personal_story tweet about coding\", \"response\": \"Spent 2 hours debugging a typo. It was a missing semicolon üòÖ\"},\n",
        "        {\"instruction\": \"Write a classic tweet about wisdom\", \"response\": \"The most dangerous phrase in programming: 'Just a small change'\"},\n",
        "        {\"instruction\": \"Write a funny tweet about technology\", \"response\": \"My computer is so slow, it's still processing my thoughts from yesterday\"},\n",
        "        {\"instruction\": \"Write a motivational tweet about learning\", \"response\": \"Every expert was once a beginner. Keep coding! üí™\"},\n",
        "        {\"instruction\": \"Write a relatable tweet about work\", \"response\": \"Me: I'll just fix this one small bug. Also me: 3 hours later...\"}\n",
        "    ]\n",
        "    \n",
        "    # Save sample data\n",
        "    with open(dataset_path, 'w') as f:\n",
        "        for item in sample_data:\n",
        "            f.write(json.dumps(item) + '\\n')\n",
        "    \n",
        "    data_path = dataset_path\n",
        "    print(f\"‚úÖ Created sample dataset: {dataset_path}\")\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"json\", data_files=data_path)\n",
        "print(f\"üìä Dataset loaded: {len(dataset['train'])} examples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "print(f\"ü§ñ Loading model: {model_name}\")\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Set pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"‚úÖ Model loaded and moved to {device}\")\n",
        "print(f\"üìè Model parameters: {model.num_parameters():,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_dataset(examples):\n",
        "    \"\"\"Format the dataset for the model\"\"\"\n",
        "    texts = [\n",
        "        inst + \"\\nResponse: \" + resp\n",
        "        for inst, resp in zip(examples[\"instruction\"], examples[\"response\"])\n",
        "    ]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Pre-format the dataset and remove all other columns\n",
        "print(\"üîÑ Formatting dataset...\")\n",
        "formatted_dataset = dataset[\"train\"].map(\n",
        "    format_dataset, \n",
        "    batched=True, \n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Formatted dataset columns: {formatted_dataset.column_names}\")\n",
        "print(f\"üìù First example: {formatted_dataset[0]}\")\n",
        "print(f\"üìä Total examples: {len(formatted_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Weights & Biases Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize W&B\n",
        "wandb.init(\n",
        "    project=\"rlhf-learning-sft\",\n",
        "    name=\"tweet-generation-sft-colab\",\n",
        "    config={\n",
        "        \"model_name\": model_name,\n",
        "        \"dataset_size\": len(formatted_dataset),\n",
        "        \"num_epochs\": 1,\n",
        "        \"batch_size\": 4,\n",
        "        \"gradient_accumulation_steps\": 4,\n",
        "        \"learning_rate\": 5e-5,\n",
        "        \"warmup_steps\": 100,\n",
        "        \"max_length\": 512,\n",
        "        \"device\": str(device),\n",
        "        \"cuda_available\": torch.cuda.is_available(),\n",
        "        \"mps_available\": torch.backends.mps.is_available(),\n",
        "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Apple Silicon (MPS)\" if torch.backends.mps.is_available() else \"CPU\",\n",
        "        \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else None,\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"‚úÖ W&B initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./sft_results\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_steps=100,\n",
        "    logging_steps=10,\n",
        "    save_steps=2000,\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=False,\n",
        "    report_to=\"wandb\",\n",
        "    run_name=\"sft_tweet_generation\",\n",
        "    logging_dir=\"./logs\",\n",
        "    # Colab-specific optimizations\n",
        "    dataloader_pin_memory=False,  # Reduce memory usage\n",
        "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training arguments configured!\")\n",
        "print(f\"üìä Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"üíæ Output directory: {training_args.output_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trainer Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SFT Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=formatted_dataset,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ SFT Trainer configured!\")\n",
        "print(f\"üéØ Training dataset size: {len(formatted_dataset)}\")\n",
        "print(f\"üîÑ Total training steps: {len(formatted_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "print(\"üöÄ Starting SFT training...\")\n",
        "print(\"üìä Check your W&B dashboard for real-time metrics!\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"‚úÖ Training completed successfully!\")\n",
        "print(\"üìÅ Check the sft_results folder for saved models\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup and Finalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Finish W&B run\n",
        "wandb.finish()\n",
        "\n",
        "print(\"üéâ Training session completed!\")\n",
        "print(\"üìä Check your W&B dashboard for detailed results\")\n",
        "print(\"üíæ Model checkpoints saved in ./sft_results/\")\n",
        "\n",
        "# Display final model info\n",
        "print(f\"\\nüìà Final model info:\")\n",
        "print(f\"   Device: {device}\")\n",
        "print(f\"   Parameters: {model.num_parameters():,}\")\n",
        "print(f\"   Training examples: {len(formatted_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Trained Model (Optional)\n",
        "\n",
        "Test your fine-tuned model with some sample prompts!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the trained model\n",
        "def generate_tweet(prompt, max_length=100):\n",
        "    \"\"\"Generate a tweet using the trained model\"\"\"\n",
        "    input_text = f\"{prompt}\\nResponse:\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Test with sample prompts\n",
        "test_prompts = [\n",
        "    \"Write a funny tweet about programming\",\n",
        "    \"Write a motivational tweet about learning\",\n",
        "    \"Write a personal story tweet about coding\"\n",
        "]\n",
        "\n",
        "print(\"üß™ Testing the trained model...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    print(f\"\\nüìù Prompt: {prompt}\")\n",
        "    response = generate_tweet(prompt)\n",
        "    print(f\"ü§ñ Generated: {response}\")\n",
        "    print(\"-\" * 30)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
