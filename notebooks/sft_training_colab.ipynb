{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YW7leC3ZXAcJ"
   },
   "source": [
    "# SFT Training for Tweet Generation\n",
    "\n",
    "This notebook implements Supervised Fine-Tuning (SFT) for tweet generation using GPT-2.\n",
    "\n",
    "## Setup and Installation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6cY28GOPXAcK",
    "outputId": "89ddd535-e613-4493-baf1-13e704250874"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/564.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m \u001b[32m563.2/564.6 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m564.6/564.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u2705 Packages installed and imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -q transformers datasets trl wandb accelerate\n",
    "\n",
    "# Import libraries\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"\u2705 Packages installed and imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbJWWRSIXAcL"
   },
   "source": [
    "## GPU Setup and Device Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QRaH4MpcXAcL",
    "outputId": "2a39fabf-6542-49a5-ddd9-4f8a8bf8e310"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83d\ude80 Using GPU: Tesla T4\n",
      "\ud83d\udcca GPU Memory: 15.8 GB\n",
      "\ud83d\udd27 CUDA Version: 12.6\n",
      "\n",
      "\ud83c\udfaf Selected device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"\ud83d\ude80 Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"\ud83d\udcca GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"\ud83d\udd27 CUDA Version: {torch.version.cuda}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"\ud83c\udf4e Using Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"\ud83d\udcbb Using CPU (training will be slower)\")\n",
    "\n",
    "# In Colab, select T4 GPU\n",
    "print(f\"\\n\ud83c\udfaf Selected device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGphlnRCXAcL"
   },
   "source": [
    "## Data Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Upload the dataset (not in Github)\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "CrUm_q8rY20T",
    "outputId": "d92accc8-8c87-4f82-a031-91eb8bb8392e"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-b099d4d2-e840-4cf8-b383-0338f3c57b31\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-b099d4d2-e840-4cf8-b383-0338f3c57b31\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saving tweet_sft_dataset_10k.jsonl to tweet_sft_dataset_10k.jsonl\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "referenced_widgets": [
      "863417a03e074517bba3ba65383ea366",
      "b5e634280b3444f4895ecbd7814e7dad",
      "eac76423851e49e59972d2816d3caf6b",
      "fe078ced4a274de3bd6d3f9650d58b4f",
      "f45c1ee28c88467ca289ac88b32df7e9",
      "e6bbf86626c8470093e3de29c1acd87d",
      "d86d2ff8126547ae8acd87bf99b270fe",
      "1109d0d56fb9418bb7c55a2b6b779287",
      "72aa9247280e468ca12510dd397dea34",
      "5b091ca9891f4362906b24a656cf3720",
      "e2cfe2aea7f14887b390caa784a0b80c"
     ]
    },
    "id": "2hGnAcnZXAcL",
    "outputId": "70a35146-082f-4cca-b1a0-f81a2a0920e1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Found dataset: tweet_sft_dataset_10k.jsonl\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "863417a03e074517bba3ba65383ea366"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83d\udcca Dataset loaded: 10000 examples\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"tweet_sft_dataset_10k.jsonl\"\n",
    "\n",
    "# Check if dataset file exists\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"\u2705 Found dataset: {dataset_path}\")\n",
    "    data_path = dataset_path\n",
    "else:\n",
    "    print(\"\ud83d\udcdd Creating sample dataset for testing...\")\n",
    "    # Create a small sample dataset (AI-suggested)\n",
    "    sample_data = [\n",
    "        {\"instruction\": \"Write a personal_story tweet about coding\", \"response\": \"Spent 2 hours debugging a typo. It was a missing semicolon \ud83d\ude05\"},\n",
    "        {\"instruction\": \"Write a classic tweet about wisdom\", \"response\": \"The most dangerous phrase in programming: 'Just a small change'\"},\n",
    "        {\"instruction\": \"Write a funny tweet about technology\", \"response\": \"My computer is so slow, it's still processing my thoughts from yesterday\"},\n",
    "        {\"instruction\": \"Write a motivational tweet about learning\", \"response\": \"Every expert was once a beginner. Keep coding! \ud83d\udcaa\"},\n",
    "        {\"instruction\": \"Write a relatable tweet about work\", \"response\": \"Me: I'll just fix this one small bug. Also me: 3 hours later...\"}\n",
    "    ]\n",
    "\n",
    "    # Save sample data\n",
    "    with open(dataset_path, 'w') as f:\n",
    "        for item in sample_data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "    data_path = dataset_path\n",
    "    print(f\"\u2705 Created sample dataset: {dataset_path}\")\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"json\", data_files=data_path)\n",
    "print(f\"\ud83d\udcca Dataset loaded: {len(dataset['train'])} examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBwBMq1zXAcL"
   },
   "source": [
    "## Model Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422,
     "referenced_widgets": [
      "2429e61afcd341438a0c1849848c1941",
      "ed4044eedcc44d609f9572b3ae9b1a23",
      "a5661e52a0c84b19afc3299e8f104a08",
      "be32e34820d04016961c0fadd5a1702f",
      "ece1015e383b4c39a4d8931bfe8dc03e",
      "89be90e5c56c4a53b52f842dca294b7f",
      "7490c8553e7448159c11d62f303c93f8",
      "d191c497a44c4c6ca52f2e6e721e4c8a",
      "bb33c331e9ae4f0f99c6db2c6548c7ad",
      "25dbd930a05d413c8bb04dea4bb30561",
      "22a8b13a89bb4ab1b919a07f0f51aaf5",
      "04b64ad2aa1e488c80fe93a1e91b2382",
      "424144b8d45145af878e1edd82aff985",
      "6c2fb347c6914d7cba6dfd99ef39ee59",
      "7d530e7a35824071882f7714cb9fdf10",
      "55b7173b872a4a2183b0ab15d196682c",
      "2534133383a24ae39af9c8902a9dd30e",
      "36514aea75ac4ba6a9ada01b5dff897e",
      "4081aab3721542abae1450e26e91aad3",
      "ae9d8aceefac458f91a396479ca14659",
      "65cf6e5d64f54733958507875cd8284e",
      "9d235ca6bfd843018db537fc9a3830bc",
      "0dd1e9eead964cc4acdee6eae4410f83",
      "56eb11f8a217439ebfc1657c88056256",
      "3b5626a7439543bdbfa4b20d4afddbfc",
      "afa0c8d6299b4608b967c54893369181",
      "df87d6573d3e4e128af37ec040661180",
      "90a371a067f648b79dc62170871dffe4",
      "633a1e4c3f3443dbaaca46856a45c73d",
      "e8898b931d3c478d8b903d5cc69dd4cd",
      "46a83cede92b42beba938ef59fdb9ed7",
      "71053b8dce084708917302bd76a7abd6",
      "04b9245036fd4fd3a3553c440ba367a9",
      "06481b4b477d4f4384ab0492ba77dd8d",
      "3c9562ba03e448bc96c73e8f57c0bf16",
      "99fccb1867194ee5ad8973aba507e04c",
      "9a53dd4803f04816bc16b1cbc98a9238",
      "cd88da237e524d81b39101fed8e53046",
      "95f7cf25243b4db8817be8c4005083f8",
      "37ecb9df5fbd44cdab7d69ef9f3eef00",
      "ccecc34bbdfe483693a8a9d99d6bcddf",
      "afec607664f5454cab01b5979763939a",
      "105dbf10bd15451d8c73a44709c9ca27",
      "f13a896705a84193a282312e8302aa13",
      "721590056bac488fa7baef29cd20dd12",
      "d81b321952ca4f40b304981ba2511771",
      "61f42006d1a4469e93481b32a9f6fed8",
      "36d69993dd074af29bfd7f8ea33499b5",
      "426759082ea04d08a6d6ab9d083491ce",
      "59c513969bff4cd3a2b50513c83b3e7b",
      "38ad4fa3efee4c43bdc51bf3d9534602",
      "1b0922ca64fd4627a765670403d107a3",
      "8778f8f9d7cc4b8a812ad56581c72c50",
      "f7e86360d8f644828881bbc6d11435c4",
      "e81fbf2cfd4d46858373c636ce0e1422",
      "2b75373b58d24fbd964d1eb70932aaf2",
      "d6592e5cbccf4ef08ffae0434afb457d",
      "d789195e56b24ca6812c59349c95137a",
      "002be6ac72454b0dad5779ef2c5f4e6c",
      "b5c89f0d06ad456cb232e1689649726d",
      "73fa62c1044b4077a4f45660cd779c23",
      "b2b6b2e4c9e74220a7e1899d19a9baf5",
      "0b2ab084bfb846bc832f679d1c8342db",
      "b34b37e78cde4fed8e8d7521dbe6b18e",
      "cd72fb18b5b045069470dbe2a2b7175e",
      "6824c8b840274e49afb0422273a62606",
      "b62770678d07442ab933a84486ff617b",
      "2aba26638ce64c919b4a7682f0703843",
      "567dc081fed942bb9a0e6d48d549a9ec",
      "18f185f4803e41f783893962c6141029",
      "1f93c8f45bfb45659d30f720dc3c989b",
      "c83a5e66b20749499a19c889928f6e16",
      "6a47f8dd59f54ab0b6b6740ec9d167d4",
      "4b107940d09a4895b5560546f8434e43",
      "655775ba34cb477f999a5668fe68391c",
      "74c8da1211b647949368eaa8ef3180ed",
      "95e1dcf4fae1422aaa30f6736735092a"
     ]
    },
    "id": "oVtrt1VyXAcM",
    "outputId": "f82d9b46-7c9d-4724-c5bd-456ce86ee445"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83e\udd16 Loading model: gpt2\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2429e61afcd341438a0c1849848c1941"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04b64ad2aa1e488c80fe93a1e91b2382"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0dd1e9eead964cc4acdee6eae4410f83"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06481b4b477d4f4384ab0492ba77dd8d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "721590056bac488fa7baef29cd20dd12"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2b75373b58d24fbd964d1eb70932aaf2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b62770678d07442ab933a84486ff617b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Model loaded and moved to cuda\n",
      "\ud83d\udccf Model parameters: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "print(f\"\ud83e\udd16 Loading model: {model_name}\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"\u2705 Model loaded and moved to {device}\")\n",
    "print(f\"\ud83d\udccf Model parameters: {model.num_parameters():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzsgM2FXXAcM"
   },
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dtOAx-g-XAcM",
    "outputId": "08d910ed-3e6f-43b2-ee0d-c260185b61d9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83d\udd04 Formatting dataset...\n",
      "\u2705 Formatted dataset columns: ['text']\n",
      "\ud83d\udcdd First example: {'text': 'Write a personal_story tweet about coding\\nResponse: Spent 2 hours debugging a typo. It was a missing semicolon \ud83d\ude05'}\n",
      "\ud83d\udcca Total examples: 10000\n"
     ]
    }
   ],
   "source": [
    "def format_dataset(examples):\n",
    "    \"\"\"Format the dataset for the model\"\"\"\n",
    "    texts = [\n",
    "        inst + \"\\nResponse: \" + resp\n",
    "        for inst, resp in zip(examples[\"instruction\"], examples[\"response\"])\n",
    "    ]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Pre-format the dataset and remove all other columns\n",
    "print(\"\ud83d\udd04 Formatting dataset...\")\n",
    "formatted_dataset = dataset[\"train\"].map(\n",
    "    format_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "print(f\"\u2705 Formatted dataset columns: {formatted_dataset.column_names}\")\n",
    "print(f\"\ud83d\udcdd First example: {formatted_dataset[0]}\")\n",
    "print(f\"\ud83d\udcca Total examples: {len(formatted_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Split the dataset\n",
    "train_dataset, val_dataset = formatted_dataset.train_test_split(test_size=1000, seed=42).values()\n",
    "\n",
    "print(f\"\ud83d\udcca Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"\ud83d\udcca Validation dataset size: {len(val_dataset)}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xoAlgUA0gMKg",
    "outputId": "e96e18c7-8084-4443-86f8-00360b015a60"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83d\udcca Training dataset size: 9000\n",
      "\ud83d\udcca Validation dataset size: 1000\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-w12pG8jXAcM"
   },
   "source": [
    "## Weights & Biases Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "id": "UWz_H3QoXAcM",
    "outputId": "49e11030-845b-4b1f-b2b3-b75d6a861f48"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tweet-generation-sft-colab-3epochs-validation</strong> at: <a href='https://wandb.ai/arjunkan2003-wex/rlhf-learning-sft/runs/a16ciahi' target=\"_blank\">https://wandb.ai/arjunkan2003-wex/rlhf-learning-sft/runs/a16ciahi</a><br> View project at: <a href='https://wandb.ai/arjunkan2003-wex/rlhf-learning-sft' target=\"_blank\">https://wandb.ai/arjunkan2003-wex/rlhf-learning-sft</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20251008_015340-a16ciahi/logs</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.22.1"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251008_015913-w7hnp43i</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/arjunkan2003-wex/rlhf-learning-sft/runs/w7hnp43i' target=\"_blank\">tweet-generation-sft-colab-3epochs-validation</a></strong> to <a href='https://wandb.ai/arjunkan2003-wex/rlhf-learning-sft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/arjunkan2003-wex/rlhf-learning-sft' target=\"_blank\">https://wandb.ai/arjunkan2003-wex/rlhf-learning-sft</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/arjunkan2003-wex/rlhf-learning-sft/runs/w7hnp43i' target=\"_blank\">https://wandb.ai/arjunkan2003-wex/rlhf-learning-sft/runs/w7hnp43i</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 W&B initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize W&B\n",
    "wandb.init(\n",
    "    project=\"rlhf-learning-sft\",\n",
    "    name=\"tweet-generation-sft-colab-3epochs-validation\",\n",
    "    config={\n",
    "        \"train_size\": len(train_dataset),\n",
    "        \"val_size\": len(val_dataset),\n",
    "        \"num_epochs\": 3,\n",
    "        \"experiment\": \"validation_split\",\n",
    "        \"batch_size\": 4,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"warmup_steps\": 100,\n",
    "        \"max_length\": 512,\n",
    "        \"eval_steps\": 100,\n",
    "        \"device\": str(device),\n",
    "        \"cuda_available\": torch.cuda.is_available(),\n",
    "        \"mps_available\": torch.backends.mps.is_available(),\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Apple Silicon (MPS)\" if torch.backends.mps.is_available() else \"CPU\",\n",
    "        \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else None,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\u2705 W&B initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgbtuRqVXAcM"
   },
   "source": [
    "## Training Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z90GfT72XAcM",
    "outputId": "e38dbb84-343a-4925-ca68-1f354d137671"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Training arguments configured!\n",
      "\ud83d\udcca Effective batch size: 16\n",
      "\ud83d\udcbe Output directory: ../sft_results\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../sft_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "\n",
    "    # NEW: Evaluation settings\n",
    "    eval_strategy=\"steps\",  # Evaluate during training\n",
    "    eval_steps=100,  # Evaluate every 100 steps\n",
    "    save_steps=2000,\n",
    "\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",  # Use validation loss\n",
    "\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"exp2-validation-split-3epochs\",\n",
    "    logging_dir=\"../logs\",\n",
    ")\n",
    "\n",
    "print(\"\u2705 Training arguments configured!\")\n",
    "print(f\"\ud83d\udcca Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"\ud83d\udcbe Output directory: {training_args.output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8m2db2-XAcM"
   },
   "source": [
    "## Trainer Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Meo9ttd4XAcM",
    "outputId": "eda927ca-0dce-432f-fde7-dea4d059185b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 SFT Trainer configured!\n",
      "\ud83c\udfaf Training dataset size: 9000\n",
      "\ud83d\udd04 Total training steps: 1686\n"
     ]
    }
   ],
   "source": [
    "# SFT Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"\u2705 SFT Trainer configured!\")\n",
    "print(f\"\ud83c\udfaf Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"\ud83d\udd04 Total training steps: {len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv35iZ9bXAcN"
   },
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "id": "GFCfT_mFXAcN",
    "outputId": "584721bc-4528-4df8-9351-2c2b82d658e6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83d\ude80 Starting SFT training...\n",
      "\ud83d\udcca Check your W&B dashboard for real-time metrics!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1689' max='1689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1689/1689 08:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.805300</td>\n",
       "      <td>0.678122</td>\n",
       "      <td>1.010408</td>\n",
       "      <td>40561.000000</td>\n",
       "      <td>0.824944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.455000</td>\n",
       "      <td>0.374534</td>\n",
       "      <td>0.665175</td>\n",
       "      <td>81332.000000</td>\n",
       "      <td>0.870371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.399400</td>\n",
       "      <td>0.356035</td>\n",
       "      <td>0.594075</td>\n",
       "      <td>122015.000000</td>\n",
       "      <td>0.873447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.346800</td>\n",
       "      <td>0.331604</td>\n",
       "      <td>0.580660</td>\n",
       "      <td>162920.000000</td>\n",
       "      <td>0.872608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.329700</td>\n",
       "      <td>0.331336</td>\n",
       "      <td>0.570523</td>\n",
       "      <td>203465.000000</td>\n",
       "      <td>0.873785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.323600</td>\n",
       "      <td>0.331430</td>\n",
       "      <td>0.555294</td>\n",
       "      <td>243857.000000</td>\n",
       "      <td>0.874127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.328400</td>\n",
       "      <td>0.327272</td>\n",
       "      <td>0.566601</td>\n",
       "      <td>284639.000000</td>\n",
       "      <td>0.873152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.311600</td>\n",
       "      <td>0.321753</td>\n",
       "      <td>0.563056</td>\n",
       "      <td>325624.000000</td>\n",
       "      <td>0.875432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.327300</td>\n",
       "      <td>0.318654</td>\n",
       "      <td>0.551397</td>\n",
       "      <td>366000.000000</td>\n",
       "      <td>0.873473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.319800</td>\n",
       "      <td>0.315559</td>\n",
       "      <td>0.557615</td>\n",
       "      <td>406584.000000</td>\n",
       "      <td>0.875867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.314000</td>\n",
       "      <td>0.313465</td>\n",
       "      <td>0.546805</td>\n",
       "      <td>447210.000000</td>\n",
       "      <td>0.874648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.302400</td>\n",
       "      <td>0.312391</td>\n",
       "      <td>0.552052</td>\n",
       "      <td>487924.000000</td>\n",
       "      <td>0.875920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.297100</td>\n",
       "      <td>0.311060</td>\n",
       "      <td>0.545896</td>\n",
       "      <td>528263.000000</td>\n",
       "      <td>0.876415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.300800</td>\n",
       "      <td>0.309912</td>\n",
       "      <td>0.535260</td>\n",
       "      <td>569073.000000</td>\n",
       "      <td>0.875340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.328000</td>\n",
       "      <td>0.307784</td>\n",
       "      <td>0.543714</td>\n",
       "      <td>609652.000000</td>\n",
       "      <td>0.876674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.312000</td>\n",
       "      <td>0.306629</td>\n",
       "      <td>0.544596</td>\n",
       "      <td>650284.000000</td>\n",
       "      <td>0.876742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Training completed successfully!\n",
      "\ud83d\udcc1 Check the sft_results folder for saved models\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"\ud83d\ude80 Starting SFT training...\")\n",
    "print(\"\ud83d\udcca Check your W&B dashboard for real-time metrics!\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\u2705 Training completed successfully!\")\n",
    "print(\"\ud83d\udcc1 Check the sft_results folder for saved models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rc4x7n3QXAcN"
   },
   "source": [
    "## Cleanup and Finalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V82iHqExXAcN",
    "outputId": "ede69132-c376-4649-b2f4-ba59d65f97c4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83c\udf89 Training session completed!\n",
      "\ud83d\udcca Check your W&B dashboard for detailed results\n",
      "\ud83d\udcbe Model checkpoints saved in ./sft_results/\n",
      "\n",
      "\ud83d\udcc8 Final model info:\n",
      "   Device: cuda\n",
      "   Parameters: 124,439,808\n",
      "   Training examples: 10000\n"
     ]
    }
   ],
   "source": [
    "# Finish W&B run\n",
    "wandb.finish()\n",
    "\n",
    "print(\"\ud83c\udf89 Training session completed!\")\n",
    "print(\"\ud83d\udcca Check your W&B dashboard for detailed results\")\n",
    "print(\"\ud83d\udcbe Model checkpoints saved in ./sft_results/\")\n",
    "\n",
    "# Display final model info\n",
    "print(f\"\\n\ud83d\udcc8 Final model info:\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Parameters: {model.num_parameters():,}\")\n",
    "print(f\"   Training examples: {len(formatted_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4cC7g5kXAcN"
   },
   "source": [
    "## Test the Trained Model (Optional)\n",
    "\n",
    "Test your fine-tuned model with some sample prompts!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wQe4euRmXAcN",
    "outputId": "8174eb5b-fe61-40a5-f9f4-f28b0a6f0228"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83e\uddea Testing the trained model...\n",
      "==================================================\n",
      "\n",
      "\ud83d\udcdd Prompt: Write a funny tweet about programming\n",
      "\ud83e\udd16 Generated: Write a funny tweet about programming\n",
      "Response: Just realized that debugging is 90% reading your own code and wondering who wrote this garbage \ud83e\udd14\n",
      "------------------------------\n",
      "\n",
      "\ud83d\udcdd Prompt: Write a motivational tweet about learning\n",
      "\ud83e\udd16 Generated: Write a motivational tweet about learning\n",
      "Response: Learning machine learning is like learning a new language - confusing until it makes sense\n",
      "------------------------------\n",
      "\n",
      "\ud83d\udcdd Prompt: Write a personal story tweet about coding\n",
      "\ud83e\udd16 Generated: Write a personal story tweet about coding\n",
      "Response: Spent 2 hours chasing down an error. It was a missing semicolon \ud83d\ude05\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the trained model\n",
    "def generate_tweet(prompt, max_length=100):\n",
    "    \"\"\"Generate a tweet using the trained model\"\"\"\n",
    "    input_text = f\"{prompt}\\nResponse:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Test with sample prompts\n",
    "test_prompts = [\n",
    "    \"Write a funny tweet about programming\",\n",
    "    \"Write a motivational tweet about learning\",\n",
    "    \"Write a personal story tweet about coding\"\n",
    "]\n",
    "\n",
    "print(\"\ud83e\uddea Testing the trained model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n\ud83d\udcdd Prompt: {prompt}\")\n",
    "    response = generate_tweet(prompt)\n",
    "    print(f\"\ud83e\udd16 Generated: {response}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the base GPT-2 model for comparison\n",
    "print(\"\ud83e\udd16 Loading base GPT-2 model for comparison...\")\n",
    "base_model_name = \"gpt2\"\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "\n",
    "# Set pad token for base tokenizer\n",
    "if base_tokenizer.pad_token is None:\n",
    "    base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "\n",
    "# Move base model to device\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "print(\"\u2705 Base GPT-2 model loaded.\")\n",
    "\n",
    "\n",
    "def generate_tweet_base(prompt, max_length=100):\n",
    "    \"\"\"Generate a tweet using the base model\"\"\"\n",
    "    input_text = f\"{prompt}\\nResponse:\"\n",
    "    inputs = base_tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = base_model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=base_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    generated_text = base_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Test with sample prompts using the base model\n",
    "test_prompts = [\n",
    "    \"Write a funny tweet about programming\",\n",
    "    \"Write a motivational tweet about learning\",\n",
    "    \"Write a personal story tweet about coding\"\n",
    "]\n",
    "\n",
    "print(\"\\n\ud83e\uddea Testing the BASE model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n\ud83d\udcdd Prompt: {prompt}\")\n",
    "    response = generate_tweet_base(prompt)\n",
    "    print(f\"\ud83e\udd16 Generated (Base Model): {response}\")\n",
    "    print(\"-\" * 30)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t6HGpZ4TbCiX",
    "outputId": "99059c80-2b8e-4bc9-bd0e-6ea1daa563bd"
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83e\udd16 Loading base GPT-2 model for comparison...\n",
      "\u2705 Base GPT-2 model loaded.\n",
      "\n",
      "\ud83e\uddea Testing the BASE model...\n",
      "==================================================\n",
      "\n",
      "\ud83d\udcdd Prompt: Write a funny tweet about programming\n",
      "\ud83e\udd16 Generated (Base Model): Write a funny tweet about programming\n",
      "Response: @joe_larsen: I'm not sure if you're aware that I'm a programmer, but I'm a very busy person and I've been doing a lot of research on programming languages. I'm a bit obsessed with programming, and I think I might have gotten a little too obsessed with it because I was really excited about it. But I'm not sure if you realize what's going on. It's a really big gap.\n",
      "------------------------------\n",
      "\n",
      "\ud83d\udcdd Prompt: Write a motivational tweet about learning\n",
      "\ud83e\udd16 Generated (Base Model): Write a motivational tweet about learning\n",
      "Response:\n",
      "\"I love math. I love being able to see what I want to see, understand it, and explain it to others. I don't usually do this, but I enjoy doing it. And when I do it, it is really great.\"\n",
      "Response:\n",
      "\"That's really nice and I'll do that again.\"\n",
      "------------------------------\n",
      "\n",
      "\ud83d\udcdd Prompt: Write a personal story tweet about coding\n",
      "\ud83e\udd16 Generated (Base Model): Write a personal story tweet about coding\n",
      "Response:\n",
      "Hi @theday. I'm not a programmer so I'm going to write a personal story about coding. I've been teaching myself programming so I can write something simple but useful code. I'm not really a big fan of the language but I want to make it easy to learn and keep things interesting by simply writing a simple code. I've read some of your stuff and I know that there are a lot of people out there that\n",
      "------------------------------\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}